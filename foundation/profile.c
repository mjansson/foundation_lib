/* profile.c  -  Foundation library  -  Public Domain  -  2013 Mattias Jansson
 *
 * This library provides a cross-platform foundation library in C11 providing basic support
 * data types and functions to write applications and games in a platform-independent fashion.
 * The latest source code is always available at
 *
 * https://github.com/mjansson/foundation_lib
 *
 * This library is put in the public domain; you can redistribute it and/or modify it without
 * any restrictions.
 */

#include <foundation/foundation.h>
#include <foundation/internal.h>

#if BUILD_ENABLE_PROFILE

#define PROFILE_ENABLE_SANITY_CHECKS 0

typedef struct profile_block_data_t profile_block_data_t;
typedef struct profile_block_t profile_block_t;

#define MAX_MESSAGE_LENGTH 25

#pragma pack(push)
#pragma pack(1)

struct profile_block_data_t {
	int32_t id;
	int32_t parentid;
	uint32_t processor;
	uint32_t thread;
	tick_t start;
	tick_t end;
	char name[MAX_MESSAGE_LENGTH + 1];
};  // sizeof( profile_block_data ) == 58
FOUNDATION_STATIC_ASSERT(sizeof(profile_block_data_t) == 58, "profile_block_data_t size");

struct profile_block_t {
	profile_block_data_t data;
	uint16_t previous;
	uint16_t sibling;
	uint16_t child;
};  // sizeof( profile_block ) == 64
FOUNDATION_STATIC_ASSERT(sizeof(profile_block_t) == 64, "profile_block_t size");

#pragma pack(pop)

// Continue values generated by +1 in block split
#define PROFILE_ID_ENDOFSTREAM 0
#define PROFILE_ID_SYSTEMINFO 1
#define PROFILE_ID_LOGMESSAGE 2
//#define PROFILE_ID_LOGCONTINUE      3
#define PROFILE_ID_ENDFRAME 4
#define PROFILE_ID_TRYLOCK 5
//#define PROFILE_ID_TRYLOCKCONTINUE  6
#define PROFILE_ID_LOCK 7
//#define PROFILE_ID_LOCKCONTINUE     8
#define PROFILE_ID_UNLOCK 9
//#define PROFILE_ID_UNLOCKCONTINUE   10
#define PROFILE_ID_WAIT 11
#define PROFILE_ID_SIGNAL 12

#define GET_BLOCK(index) (profile_blocks + (index))
#define BLOCK_INDEX(block) (uint16_t)((uintptr_t)((block)-profile_blocks))

static string_const_t profile_identifier_current;
static atomic32_t profile_counter;
static atomic32_t profile_loopid;
static atomic32_t profile_free;
static atomic32_t profile_root;
static profile_block_t* profile_blocks;
static tick_t profile_ground_time;
static int profile_enabled;
static profile_write_fn profile_write;
static uint32_t profile_block_count;
static unsigned int profile_wait_time = 100;
static thread_t profile_io_thread;
static bool profile_initialized;
static atomic32_t profile_has_warned;

FOUNDATION_DECLARE_THREAD_LOCAL(int32_t, profile_block, 0)

static profile_block_t*
profile_allocate_block(void) {
	/*lint --e{701} */
	// Grab block from free list, avoiding ABA issues by
	// using high 16 bit as a loop counter
	profile_block_t* block;
	int32_t free_block_tag, free_block, next_block_tag;
	do {
		free_block_tag = atomic_load32(&profile_free, memory_order_acquire);
		free_block = free_block_tag & 0xffff;

		next_block_tag = GET_BLOCK(free_block)->child;
		next_block_tag |= (atomic_incr32(&profile_loopid, memory_order_relaxed) & 0xffff) << 16;
	} while (free_block &&
	         !atomic_cas32(&profile_free, next_block_tag, free_block_tag, memory_order_release, memory_order_acquire));

	if (!free_block) {
		if (atomic_cas32(&profile_has_warned, 1, 0, memory_order_release, memory_order_acquire)) {
			if (profile_block_count < 65535)
				log_error(0, ERROR_OUT_OF_MEMORY,
				          STRING_CONST("Profile blocks exhausted, increase profile memory block size"));
			else
				log_error(0, ERROR_OUT_OF_MEMORY,
				          STRING_CONST("Profile blocks exhausted, decrease profile output wait time"));
		}
		return 0;
	}

	block = GET_BLOCK(free_block);
	memset(block, 0, sizeof(profile_block_t));
	return block;
}

static void
profile_free_block(int32_t block, int32_t leaf) {
	/*lint --e{701} */
	int32_t last_tag, block_tag;
	do {
		block_tag = block | ((atomic_incr32(&profile_loopid, memory_order_relaxed) & 0xffff) << 16);
		last_tag = atomic_load32(&profile_free, memory_order_acquire);
		GET_BLOCK(leaf)->child = last_tag & 0xffff;
	} while (!atomic_cas32(&profile_free, block_tag, last_tag, memory_order_release, memory_order_acquire));
}

static void
profile_put_root_block(int32_t block) {
	uint16_t sibling;
	profile_block_t* self = GET_BLOCK(block);

#if PROFILE_ENABLE_SANITY_CHECKS
	FOUNDATION_ASSERT(self->sibling == 0);
#endif
	while (!atomic_cas32(&profile_root, block, 0, memory_order_release, memory_order_acquire)) {
		do {
			sibling = (uint16_t)atomic_load32(&profile_root, memory_order_acquire);
		} while (sibling &&
		         !atomic_cas32(&profile_root, 0, (int32_t)sibling, memory_order_release, memory_order_acquire));

		if (sibling) {
			if (self->sibling) {
				uint16_t leaf = self->sibling;
				while (GET_BLOCK(leaf)->sibling)
					leaf = GET_BLOCK(leaf)->sibling;
				GET_BLOCK(sibling)->previous = leaf;
				GET_BLOCK(leaf)->sibling = sibling;
			} else {
				self->sibling = sibling;
			}
		}
	}
}

static void
profile_put_simple_block(int32_t block) {
	// Add to current block, or if no current add to array
	int32_t parent_block = get_thread_profile_block();
	if (parent_block) {
		profile_block_t* self = GET_BLOCK(block);
		profile_block_t* parent = GET_BLOCK(parent_block);
		int32_t next_block = parent->child;
		self->previous = (uint16_t)parent_block;
		self->sibling = (uint16_t)next_block;
		if (next_block)
			GET_BLOCK(next_block)->previous = (uint16_t)block;
		parent->child = (uint16_t)block;
	} else {
		profile_put_root_block(block);
	}
}

static void
profile_put_message_block(int32_t id, const char* message, size_t length) {
	profile_block_t* subblock;

	// Allocate new master block
	profile_block_t* block = profile_allocate_block();
	if (!block)
		return;
	block->data.id = id;
	block->data.processor = thread_hardware();
	block->data.thread = (uint32_t)thread_id();
	block->data.start = time_current() - profile_ground_time;
	block->data.end = atomic_add32(&profile_counter, 1, memory_order_relaxed);
	string_copy(block->data.name, sizeof(block->data.name), message, length);

	length = (length > MAX_MESSAGE_LENGTH ? length - MAX_MESSAGE_LENGTH : 0);
	message += MAX_MESSAGE_LENGTH;
	subblock = block;

	while (length > 0) {
		// add subblock
		profile_block_t* cblock = profile_allocate_block();
		uint16_t cblock_index;
		if (!cblock)
			return;
		cblock_index = BLOCK_INDEX(cblock);
		cblock->data.id = id + 1;
		cblock->data.parentid = (int32_t)subblock->data.end;
		cblock->data.processor = block->data.processor;
		cblock->data.thread = block->data.thread;
		cblock->data.start = block->data.start;
		cblock->data.end = atomic_add32(&profile_counter, 1, memory_order_relaxed);
		string_copy(cblock->data.name, sizeof(cblock->data.name), message, length);

		cblock->sibling = subblock->child;
		if (cblock->sibling)
			GET_BLOCK(cblock->sibling)->previous = cblock_index;
		subblock->child = cblock_index;
		cblock->previous = BLOCK_INDEX(subblock);
		subblock = cblock;

		length = (length > MAX_MESSAGE_LENGTH ? length - MAX_MESSAGE_LENGTH : 0);
		message += MAX_MESSAGE_LENGTH;
	}

	profile_put_simple_block(BLOCK_INDEX(block));
}

// Pass each block once, writing it to stream and adjusting child/sibling pointers to form
// a single-linked list through child pointer. Potential drawback of this is that block access
// order will degenerate over time and result in random access over the whole profile memory
// area in the end
static profile_block_t*
profile_process_block(profile_block_t* block) {
	profile_block_t* leaf = block;

	if (profile_write)
		profile_write(block, sizeof(profile_block_t));

	if (block->child) {
		leaf = profile_process_block(GET_BLOCK(block->child));
		if (block->sibling) {
			profile_block_t* subleaf = profile_process_block(GET_BLOCK(block->sibling));
			subleaf->child = block->child;
			block->child = block->sibling;
			block->sibling = 0;
		}
	} else if (block->sibling) {
		leaf = profile_process_block(GET_BLOCK(block->sibling));
		block->child = block->sibling;
		block->sibling = 0;
	}
	return leaf;
}

static void
profile_process_root_block(void) {
	int32_t block;

	do {
		block = atomic_load32(&profile_root, memory_order_acquire);
	} while (block && !atomic_cas32(&profile_root, 0, block, memory_order_release, memory_order_acquire));

	while (block) {
		profile_block_t* leaf;
		profile_block_t* current = GET_BLOCK(block);
		int32_t next = current->sibling;

		current->sibling = 0;
		leaf = profile_process_block(current);
		profile_free_block(block, BLOCK_INDEX(leaf));

		block = next;
	}
}

static void*
profile_io(void* arg) {
	unsigned int system_info_counter = 0;
	profile_block_t system_info;
	FOUNDATION_UNUSED(arg);
	memset(&system_info, 0, sizeof(profile_block_t));
	system_info.data.id = PROFILE_ID_SYSTEMINFO;
	system_info.data.start = time_ticks_per_second();
	string_copy(system_info.data.name, sizeof(system_info.data.name), "sysinfo", 7);

	while (!thread_try_wait(profile_wait_time)) {
		if (!atomic_load32(&profile_root, memory_order_acquire))
			continue;

		profile_begin_block(STRING_CONST("profile_io"));

		if (atomic_load32(&profile_root, memory_order_acquire)) {
			profile_begin_block(STRING_CONST("process"));

			// This is thread safe in the sense that only completely closed and ended
			// blocks will be put as children to root block, so no additional blocks
			// will ever be added to child subtrees while we process it here
			profile_process_root_block();

			profile_end_block();
		}

		if (system_info_counter++ > 10) {
			if (profile_write)
				profile_write(&system_info, sizeof(profile_block_t));
			system_info_counter = 0;
		}

		profile_end_block();
	}

	if (atomic_load32(&profile_root, memory_order_acquire))
		profile_process_root_block();

	if (profile_write) {
		profile_block_t terminate;
		memset(&terminate, 0, sizeof(profile_block_t));
		terminate.data.id = PROFILE_ID_ENDOFSTREAM;
		profile_write(&terminate, sizeof(profile_block_t));
	}

	return 0;
}

void
profile_initialize(const char* identifier, size_t length, void* buffer, size_t capacity) {
	profile_block_t* root = buffer;
	profile_block_t* block = root;
	uint32_t block_count = (uint32_t)(capacity / sizeof(profile_block_t));
	uint32_t i;

	if (block_count > 65535)
		block_count = 65535;

	for (i = 0; i < (block_count - 1); ++i, ++block) {
		block->child = (uint16_t)(i + 1);
		block->sibling = 0;
	}
	block->child = 0;
	block->sibling = 0;
	root->child = 0;

	atomic_store32(&profile_root, 0, memory_order_relaxed);

	profile_block_count = block_count;
	profile_identifier_current = string_const(identifier, length);
	profile_blocks = root;
	// TODO: Currently 0 is a no-block identifier, so we waste the first block
	atomic_store32(&profile_free, 1, memory_order_relaxed);
	atomic_store32(&profile_counter, 128, memory_order_relaxed);
	profile_ground_time = time_current();
	set_thread_profile_block(0);
	atomic_thread_fence_release();

	thread_initialize(&profile_io_thread, profile_io, 0, STRING_CONST("profile_io"), THREAD_PRIORITY_BELOWNORMAL, 0);

	profile_initialized = true;
}

void
profile_finalize(void) {
	if (!profile_initialized)
		return;

	profile_enable(0);

	thread_signal(&profile_io_thread);
	thread_finalize(&profile_io_thread);

	// Discard and free up blocks remaining in queue
	profile_thread_finalize();
	if (atomic_load32(&profile_root, memory_order_acquire))
		profile_process_root_block();

	// Sanity checks
	{
		uint32_t block_count = 0;
		uint32_t free_block = atomic_load32(&profile_free, memory_order_acquire) & 0xffff;

		if (atomic_load32(&profile_root, memory_order_acquire))
			log_error(0, ERROR_INTERNAL_FAILURE,
			          STRING_CONST("Profile module state inconsistent on finalize, "
			                       "at least one root block still allocated/active"));

		while (free_block) {
			profile_block_t* block = GET_BLOCK(free_block);
			if (block->sibling)
				log_errorf(0, ERROR_INTERNAL_FAILURE,
				           STRING_CONST("Profile module state inconsistent on finalize, "
				                        "block %d has sibling set"),
				           free_block);
			++block_count;
			free_block = GET_BLOCK(free_block)->child;
		}
		if (profile_block_count)
			++block_count;  // Include the wasted block 0

		if (block_count != profile_block_count) {
			// If profile output function (user) raised exception, this will probably trigger
			// since at least one block will be lost in space
			log_errorf(0, ERROR_INTERNAL_FAILURE,
			           STRING_CONST("Profile module state inconsistent on finalize, lost blocks "
			                        "(found %u of %u)"),
			           block_count, profile_block_count);
		}
	}

	atomic_store32(&profile_root, 0, memory_order_relaxed);
	atomic_store32(&profile_free, 0, memory_order_relaxed);

	profile_block_count = 0;
	profile_identifier_current = string_null();
	profile_initialized = false;
}

void
profile_set_output(profile_write_fn writer) {
	profile_write = writer;
}

void
profile_set_output_wait(unsigned int ms) {
	profile_wait_time = (ms ? ms : 1U);
}

void
profile_enable(bool enable) {
	bool was_enabled = (profile_enabled > 0);
	bool is_enabled = enable;

	if (!profile_initialized)
		return;

	if (is_enabled && !was_enabled) {
		// Start output thread
		profile_enabled = 1;
		thread_start(&profile_io_thread);
	} else if (!is_enabled && was_enabled) {
		// Stop output thread
		thread_signal(&profile_io_thread);
		thread_join(&profile_io_thread);
		profile_enabled = 0;
	}
}

void
profile_end_frame(uint64_t counter) {
	profile_block_t* block;
	if (!profile_enabled)
		return;

	// Allocate new master block
	block = profile_allocate_block();
	if (!block)
		return;
	block->data.id = PROFILE_ID_ENDFRAME;
	block->data.processor = thread_hardware();
	block->data.thread = (uint32_t)thread_id();
	block->data.start = time_current() - profile_ground_time;
	block->data.end = (tick_t)counter;

	profile_put_simple_block(BLOCK_INDEX(block));
}

void
profile_begin_block(const char* message, size_t length) {
	int32_t parent;
	if (!profile_enabled)
		return;

	parent = get_thread_profile_block();
	if (!parent) {
		// Allocate new master block
		profile_block_t* block = profile_allocate_block();
		uint16_t blockindex;
		if (!block)
			return;
		blockindex = BLOCK_INDEX(block);
		block->data.id = atomic_add32(&profile_counter, 1, memory_order_relaxed);
		string_copy(block->data.name, sizeof(block->data.name), message, length);
		block->data.processor = thread_hardware();
		block->data.thread = (uint32_t)thread_id();
		block->data.start = time_current() - profile_ground_time;
		set_thread_profile_block(blockindex);
	} else {
		// Allocate new child block
		profile_block_t* parentblock;
		profile_block_t* subblock = profile_allocate_block();
		uint16_t subindex;
		if (!subblock)
			return;
		subindex = BLOCK_INDEX(subblock);
		parentblock = GET_BLOCK(parent);
		subblock->data.id = atomic_add32(&profile_counter, 1, memory_order_relaxed);
		subblock->data.parentid = parentblock->data.id;
		string_copy(subblock->data.name, sizeof(subblock->data.name), message, length);
		subblock->data.processor = thread_hardware();
		subblock->data.thread = (uint32_t)thread_id();
		subblock->data.start = time_current() - profile_ground_time;
		subblock->previous = (uint16_t)parent;
		subblock->sibling = parentblock->child;
		if (parentblock->child)
			GET_BLOCK(parentblock->child)->previous = subindex;
		parentblock->child = subindex;
		set_thread_profile_block(subindex);
	}
}

void
profile_update_block(void) {
	char* message;
	unsigned int processor;
	int32_t block_index = get_thread_profile_block();
	profile_block_t* block;
	if (!profile_enabled || !block_index)
		return;

	block = GET_BLOCK(block_index);
	message = block->data.name;
	processor = thread_hardware();
	if (block->data.processor == processor)
		return;

	// Thread migrated to another core, split into new block
	profile_end_block();
	profile_begin_block(message, string_length(message));
}

void
profile_end_block(void) {
	int32_t block_index = get_thread_profile_block();
	profile_block_t* block;
	if (!profile_enabled || !block_index)
		return;

	block = GET_BLOCK(block_index);
	block->data.end = time_current() - profile_ground_time;

	if (block->previous) {
		unsigned int processor;
		profile_block_t* current = block;
		profile_block_t* previous = GET_BLOCK(block->previous);
		profile_block_t* parent;
		int32_t current_index = block_index;
		uint16_t parent_index;
		while (previous->child != current_index) {
			current_index = current->previous;  // Walk sibling list backwards
			current = GET_BLOCK(current_index);
			previous = GET_BLOCK(current->previous);
#if PROFILE_ENABLE_SANITY_CHECKS
			FOUNDATION_ASSERT(current_index != 0);
			FOUNDATION_ASSERT(current->previous != 0);
#endif
		}
		parent_index = current->previous;  // Previous now points to parent
		parent = GET_BLOCK(parent_index);
#if PROFILE_ENABLE_SANITY_CHECKS
		FOUNDATION_ASSERT(parent_index != block_index);
#endif
		set_thread_profile_block(parent_index);

		processor = thread_hardware();
		if (parent->data.processor != processor) {
			const char* message = parent->data.name;
			// Thread migrated, split into new block
			profile_end_block();
			profile_begin_block(message, string_length(message));
		}
	} else {
		profile_put_root_block(block_index);
		set_thread_profile_block(0);
	}
}

void
profile_log(const char* message, size_t length) {
	if (!profile_enabled)
		return;
	profile_put_message_block(PROFILE_ID_LOGMESSAGE, message, length);
}

void
profile_trylock(const char* name, size_t length) {
	if (!profile_enabled)
		return;
	profile_put_message_block(PROFILE_ID_TRYLOCK, name, length);
}

void
profile_lock(const char* name, size_t length) {
	if (!profile_enabled)
		return;
	profile_put_message_block(PROFILE_ID_LOCK, name, length);
}

void
profile_unlock(const char* name, size_t length) {
	if (!profile_enabled)
		return;
	profile_put_message_block(PROFILE_ID_UNLOCK, name, length);
}

void
profile_wait(const char* name, size_t length) {
	if (!profile_enabled)
		return;
	profile_put_message_block(PROFILE_ID_WAIT, name, length);
}

void
profile_signal(const char* name, size_t length) {
	if (!profile_enabled)
		return;
	profile_put_message_block(PROFILE_ID_SIGNAL, name, length);
}

string_const_t
profile_identifier(void) {
	return profile_identifier_current;
}

#endif

void
profile_thread_finalize(void) {
#if BUILD_ENABLE_PROFILE
	int32_t block_index, last_block = 0;
	while ((block_index = get_thread_profile_block())) {
		log_warnf(0, WARNING_SUSPICIOUS, STRING_CONST("Profile thread cleanup, free block %u"), block_index);
		if (last_block == block_index) {
			log_warnf(0, WARNING_SUSPICIOUS, STRING_CONST("Unrecoverable error, self reference in block %u"),
			          block_index);
			break;
		}
		profile_end_block();
		last_block = block_index;
	}
#endif
}
